I"¶
<h1 id="recast-interactive-auditing-of-automatic-toxicity-detection-models">RECAST: Interactive Auditing of Automatic Toxicity Detection Models</h1>
<p>

  
    
<a href="http://austinpwright.com">Austin P. Wright</a>,
  

  
    
<a href="http://oshaikh.com/">Omar Shaikh</a>,
  

  
    
<a href="https://haekyu.com">Haekyu Park</a>,
  

  
    
<a href="http://www.willepperson.com">Will Epperson</a>,
  

  
    
Muhammed Ahmed,
  

  
    
Stephane Pinel,
  

  
    
<a href="https://www.cc.gatech.edu/~dyang888/">Diyi Yang</a>,
  

  
    
<a href="https://www.cc.gatech.edu/~dchau">Duen Horng (Polo) Chau</a>
  

</p>

<figure>
    <img class="single" src="/images/papers/20-recast-chi.png" />
    <figcaption class="single">
      A: RECAST consists of a textbox and a radial progress bar. A color change on the radial progress, along with a score, indicate the toxicity of a sentence. 
Toxicity ranges from white (non-toxic) to red (very toxic). Users can hover over options to preview toxicity scores for replacing the selected word in the sentence. 
B: upon replacing the word (in the case of this figure, replacing ‚Äúidiotic‚Äù with ‚Äúnonsensical‚Äù), the main radial progress bar reflects the reduced toxicity score. 
However the small attention on the other pejorative word "moron" compared to "video" in the alternative version shows the idiosyncrasies of the model and underlying dataset.


    </figcaption>
</figure>

<h2 id="abstract">Abstract</h2>
<p>As toxic language becomes nearly pervasive online, there has been increasing interest in leveraging the advancements in natural language processing (NLP) to automatically detect and remove toxic comments. 
Despite fairness concerns and limited interpretability, there is currently little work for auditing these systems in particular for end users. 
We present our ongoing work, RECAST, an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech. 
RECAST displays the attention of toxicity detection models on user input, and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space. 
Finally we propose a larger user study  of RECAST, with promising preliminary results, to validate it‚Äôs effectiveness and useability with end users.</p>

<h2 id="citation">Citation</h2>
<p>

	<strong>RECAST: Interactive Auditing of Automatic Toxicity Detection Models</strong>


<br />


	
		
<a href="http://austinpwright.com">Austin P. Wright</a>,
	

	
		
<a href="http://oshaikh.com/">Omar Shaikh</a>,
	

	
		
<a href="https://haekyu.com">Haekyu Park</a>,
	

	
		
<a href="http://www.willepperson.com">Will Epperson</a>,
	

	
		
Muhammed Ahmed,
	

	
		
Stephane Pinel,
	

	
		
<a href="https://www.cc.gatech.edu/~dyang888/">Diyi Yang</a>,
	

	
		
<a href="https://www.cc.gatech.edu/~dchau">Duen Horng (Polo) Chau</a>
	


<br />

<i>The eighth International Workshop of Chinese CHI.  2020.</i>

<br />


	<span class="pub-misc">

	

	
	  <a href="/papers/recast">
	    <i class="fas fa-link" aria-hidden="true"></i> Project
	  </a>
	

	

	
	  <a href="https://arxiv.org/pdf/2001.01819.pdf">
	    <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
	  </a>
	

	

	

	

	

	

	

	

	

	

	

	
		
		
	

	

	
	
	</span>







</p>

<!-- ## BibTeX
```
@article{wright2020recast,
title={RECAST: Interactive Auditing of Automatic Toxicity Detection Models},
author={Austin P. Wright and Omar Shaikh and Haekyu Park and Will Epperson and Muhammed Ahmed and Stephane Pinel and Diyi Yang and Duen Horng (Polo) Chau},
year={2020},
eprint={2001.01819},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

``` -->
:ET